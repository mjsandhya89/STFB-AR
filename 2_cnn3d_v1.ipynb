{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip  install moabb\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os, moabb,sys, time,pickle\n",
    "from moabb.datasets import BNCI2014001\n",
    "import pandas as pd\n",
    "from cnn_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import scipy.io as siobn \n",
    "from moabb.pipelines.utils import FilterBank\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from pyriemann.estimation import Covariances\n",
    "\n",
    "##########\n",
    "\n",
    "# saving the results\n",
    "def save_csvfile(results,sfile_name):\n",
    "    if(os.path.isfile('./'+sfile_name) ==True):\n",
    "        print(\"exists\")\n",
    "        results.to_csv(sfile_name, mode=\"a\", index=False, header=False)\n",
    "    else:\n",
    "        print(\"No.. so creating\")\n",
    "        results.to_csv(sfile_name,  index=False, header=True) \n",
    "    return\n",
    "\n",
    "from pyriemann.tangentspace import TangentSpace,FGDA\n",
    "def fgda(X_train,X_test,y_train):\n",
    "    fb =  FilterBank(make_pipeline(Covariances(estimator=\"oas\"),FGDA()), flatten=False)\n",
    "    fb.fit(X_train,y_train)\n",
    "    xt=fb.transform(X_train);xe=fb.transform(X_test)\n",
    "    return xt,xe,y_train\n",
    "\n",
    "def load_pkl(sub,nband,fpath):\n",
    "    direc=fpath #+\"/xy/sub\"+str(sub)\n",
    "    with open(direc+'xt_cnn7.pkl', 'rb') as file:\n",
    "        xt=pickle.load(file)\n",
    "    with open(direc+'xe_cnn7.pkl', 'rb') as file:\n",
    "        xe=pickle.load(file)\n",
    "    with open(direc+'yt_cnn7.pkl', 'rb') as file:\n",
    "        yt=pickle.load(file)\n",
    "    with open(direc+'ye_cnn7.pkl', 'rb') as file:\n",
    "        ye=pickle.load(file)\n",
    "    print(xt.keys())\n",
    "    return xt[(sub,nband)],xe[(sub,nband)],yt[(sub,nband)],ye[(sub,nband)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, w2, w3, w_o, p_keep_conv, p_keep_hidden,b1,b2):\n",
    "    l1a = tf.nn.relu(tf.nn.conv3d(x, w, strides=[1, 1, 1, 1, 1], padding='VALID'))\n",
    "    l1a = tf.nn.bias_add(l1a, b1)\n",
    "    l1a = tf.nn.dropout(l1a, p_keep_conv)\n",
    "\n",
    "    l2a = tf.nn.relu(tf.nn.conv3d(l1a, w2, strides=[1, 1, 1, 1, 1], padding='VALID'))\n",
    "    l2a = tf.nn.bias_add(l2a, b2)\n",
    "    l2a = tf.nn.dropout(l2a, p_keep_conv)\n",
    "      \n",
    "\n",
    "    l2 = tf.reshape(l2a, [-1, w3.get_shape().as_list()[0]])\n",
    "    l3 = tf.nn.relu(tf.matmul(l2, w3))\n",
    "    l3 = tf.nn.dropout(l3, p_keep_hidden)\n",
    "\n",
    "    pyx = tf.matmul(l3, w_o)\n",
    "    return pyx\n",
    "\n",
    "def nn_cnn3d(trX,trY,teX,teY,itr,k1,k2,f1,f2,eta,dropout):\n",
    "    nfb=trX.shape[-1]\n",
    "    nch=trX.shape[1]\n",
    "    n_out=trY.shape[-1]\n",
    "    trX = trX.reshape(-1, nch, nch, nfb, 1) \n",
    "    teX = teX.reshape(-1, nch, nch, nfb, 1) \n",
    "\n",
    "    x = tf.placeholder(\"float\", [None, nch, nch, nfb, 1]) \n",
    "    y_ = tf.placeholder(\"float\", [None, n_out])\n",
    "\n",
    "    kernel1=k1\n",
    "    kernel2=k2\n",
    "    feature_map1=f1 #50\n",
    "    feature_map2=f2 #100\n",
    "\n",
    "    w = tf.get_variable(\"w\", shape=[kernel1, kernel1, kernel2, 1, feature_map1],\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    w2 = tf.get_variable(\"w2\", shape=[nch+1-kernel1, nch+1-kernel1, nfb+1-kernel2, feature_map1, feature_map2],\n",
    "                         initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    w3 = tf.get_variable(\"w3\", shape=[feature_map2, feature_map2], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    w_o = tf.get_variable(\"w_o\", shape=[feature_map2, n_out], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "    b1 = tf.get_variable(\"b1\", shape=[feature_map1], initializer=tf.constant_initializer(0.0))\n",
    "    b2 = tf.get_variable(\"b2\", shape=[feature_map2], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    p_keep_conv = tf.placeholder(\"float\")\n",
    "    p_keep_hidden = tf.placeholder(\"float\")\n",
    "    y = model(x, w, w2, w3, w_o, p_keep_conv, p_keep_hidden,b1,b2)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)), tf.float32))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "    train_op = tf.train.AdamOptimizer(eta).minimize(cost)\n",
    "    predict_op = tf.argmax(y, 1)\n",
    "\n",
    "    acc = [0] * itr\n",
    "    tracc = [0] * itr\n",
    "    # Launch the graph in a session\n",
    "    ypred={}\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for i in range(itr):\n",
    "            sess.run(train_op, feed_dict={x: np.array(trX), y_: np.array(trY), p_keep_conv: dropout, p_keep_hidden: dropout})\n",
    "            #print(100 * np.mean(np.argmax(teY, axis=1) == sess.run(predict_op, feed_dict={x: teX, y_: teY, p_keep_conv: 1.0, p_keep_hidden: 1.0})))\n",
    "    #         st = time.time()\n",
    "\n",
    "            ypred[i]=sess.run(predict_op, feed_dict={x: teX, y_: teY, p_keep_conv: 1.0, p_keep_hidden: 1.0})\n",
    "            acc[i] = np.mean(np.argmax(teY, axis=1) == ypred[i])\n",
    "            #en = time.time()\n",
    "            tracc[i] = sess.run(accuracy, feed_dict={x: trX, y_: trY, p_keep_conv: 0.8, p_keep_hidden: 0.8})\n",
    "            print(i,tracc[i],acc[i])\n",
    "    y_pred=ypred[np.argmax(acc)]        \n",
    "    return y_pred,ypred[i]#,y_pred #,ypred[np.argmax(acc)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbs=[\"4\"] #[\"2468\",\"4\",\"24\",\"246\"]\n",
    "subj = [ 1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "nbands_Sel=[7] #[2,3,4,5,6,8,9,10]\n",
    "mode=\"baseline_fb_fixed7_fgda\"\n",
    "dataset = BNCI2014001();dataset_name=\"BNCI2014001\"\n",
    "itr=200;k1=5;k2=5;f1=50;f2=100;eta=0.001;dropout=0.6 #itr=200;k1=5;k2=5;f1=50;f2=100;eta=0.001;dropout=0.7\n",
    "\n",
    "pipe=\"cnn3d_itr\"+str(itr)+\"_k1\"+str(k1)+\"_k2\"+str(k2)+\"_f1\"+str(f1)+\"_f2\"+str(f2)+\"_eta\"+str(eta)+\"_dropout\"+str(dropout)\n",
    "for nb in nbs:\n",
    "    fpath=\"tw_fb\"+nb+\"/\"#fpath=\"timewin_filters_all/fb\"+nb\n",
    "    fpath=\"baseline_fb/\"\n",
    "    yp={};y_act_p={}\n",
    "    for sub in subj:\n",
    "        for nband in nbands_Sel:\n",
    "            print(\"bands_hz:\",nb,\"|| subject:\",sub, \"|| nband:\",nband)\n",
    "            xt,xe,yt,ye=load_pkl(sub,nband,fpath)\n",
    "            print(xt.shape,xe.shape,yt.shape,np.unique(yt))\n",
    "            xt,xe,yt=fgda(xt,xe,yt)\n",
    "            trX = xt;      teX = xe\n",
    "            trY,teY=onehot_encode_list(yt,ye)\n",
    "            tf.reset_default_graph()\n",
    "            tf.set_random_seed(1230)\n",
    "            yp[sub],y_act_p[sub]=nn_cnn3d(trX,trY,teX,teY,itr,k1,k2,f1,f2,eta,dropout)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['dataset','session','channels','subject','n_filters','pipeline','acc',\n",
    "    'recall','precision','f1','kappa','sensitivity','specificity'])\n",
    "sfile_name = fpath+\"CNN_accResults_CrossSess_\"+mode+\"_dscore_nbands\"+nb+\".csv\"    \n",
    "nchan=xt.shape[1]\n",
    "for sub in subj:\n",
    "    acc,recall,precision,f1,kappa,sens,spec=all_metrics(ye,yp[sub])\n",
    "    df.loc[len(df)] = [pipe, \"session_E\", nchan, sub, nband, \"3DCNN\", acc,recall,precision,f1,kappa,sens,spec]\n",
    "save_csvfile(df,sfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['dataset','session','channels','subject','n_filters','pipeline','acc',\n",
    "    'recall','precision','f1','kappa','sensitivity','specificity'])\n",
    "sfile_name = fpath+\"CNN_act_accResults_CrossSess_\"+mode+\"_dscore_nbands\"+nb+\".csv\"    \n",
    "nchan=xt.shape[1]\n",
    "for sub in subj:\n",
    "    acc,recall,precision,f1,kappa,sens,spec=all_metrics(ye,y_act_p[sub])\n",
    "    df.loc[len(df)] = [pipe, \"session_E\", nchan, sub, nband, \"3DCNN\", acc,recall,precision,f1,kappa,sens,spec]\n",
    "save_csvfile(df,sfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2793c4ebbbb80d1fc165d4801e8a6c765a804c7e7ba1d5682a1ddb1afca6cdeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
